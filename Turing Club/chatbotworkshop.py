# -*- coding: utf-8 -*-
"""ChatBotWorkShop.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KUgeWmYPfE4tttRuexnfpoQKaiBHErIT

First we will import the tools we are going to use. If you came to last weeks workshop you know how an import statement works. They are the tools that will help us build the algorithm
"""

# Keras 
import keras.utils as ku 
# The Layers we will Use
from keras.layers import Embedding, LSTM, Dense, Dropout
# Tokenizer
from keras.preprocessing.text import Tokenizer
# Our model
from keras.models import Sequential
# To address overfitting
from keras.callbacks import EarlyStopping
# To enable sequencing in our data
from keras.preprocessing.sequence import pad_sequences
# Seeds for reproducibility
from tensorflow import set_random_seed
from numpy.random import seed
set_random_seed(2)
seed(1)
import re
import pandas as pd
import numpy as np
import string, os

"""Afterwards we load our data. We have to load both the lines and the conversation lines based on how the data is scattered in the dataset"""

lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\n')
conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\n')

"""You'll be able to see that the lines in themselves do not make much sense"""

lines[:10]

"""Don't pay attention to the following three cells of code. They mereley process our data and set it up in continous blocks of text"""

id2line = {}
for line in lines:
    _line = line.split(' +++$+++ ')
    if len(_line) == 5:
        id2line[_line[0]] = _line[4]

convs = [ ]
for line in conv_lines[:-1]:
    _line = line.split(' +++$+++ ')[-1][1:-1].replace("'","").replace(" ","")
    convs.append(_line.split(','))

conversations = []
for conv in convs:
    for i in range(len(conv)-1):
        conversations.append(id2line[conv[i]])

"""Now we can observer our actual text. Let's print it!"""

print(conversations[0])
print(conversations[1])

"""We will need to find a way to remove the special characters and notation from the text. So we make our clean_text function"""

def clean_text(text):
    '''Clean text by removing unnecessary characters and altering the format of words.'''

    text = text.lower()
    
    text = re.sub(r"i'm", "i am", text)
    text = re.sub(r"he's", "he is", text)
    text = re.sub(r"she's", "she is", text)
    text = re.sub(r"it's", "it is", text)
    text = re.sub(r"that's", "that is", text)
    text = re.sub(r"what's", "that is", text)
    text = re.sub(r"where's", "where is", text)
    text = re.sub(r"how's", "how is", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can't", "cannot", text)
    text = re.sub(r"n't", " not", text)
    text = re.sub(r"n'", "ng", text)
    text = re.sub(r"'bout", "about", text)
    text = re.sub(r"'til", "until", text)
    text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,]", "", text)
    
    return text

"""Now we clean our text with our function"""

clean_conversations = []
for conv in conversations:
    clean_conversations.append(clean_text(conv))

"""Let's visualize our conversations once more to see how our text looks"""

for i in range(0, 3):
    print(clean_conversations[i])

"""Now we can't really use text because machines cannot read. So we will need to use numbers instead. What we have to do is then assign each unique letter a unique number. We do this with the Keras Tokenizer."""

# Declare a tokenizer object to use
tokenizer = Tokenizer()

def get_sequences(text):
    # encode our words
    tokenizer.fit_on_texts(text)
    # how many words we have in total ( + 1 because it starts at 0)
    total_words = len(tokenizer.word_index) + 1
    ## convert data to sequence
    sequences = []
    for sentence in text:
        token_sentences = tokenizer.texts_to_sequences([sentence])[0]
        for i in range(1, len(token_sentences)):
            # For each token (word) in our sentence we create an array with the token and its previous tokens
            sequence = token_sentences[:i+1]
            # Add that sequence to our array of sequences
            sequences.append(sequence)
    # Return our total sequences and the total number of words in our text
    return sequences, total_words

"""Now let's convert them to sequences"""

token_conv, total_words = get_sequences(clean_conversations[:35000])

"""Let's now visualize how these sequences look like"""

print(token_conv[:15])

"""We can see that each word is assigned a number. But what if we have sentences that are larger than others? Our algorithm feeds of a set number of inputs so we have to generalize our sequences for them all to be the same size. We then use our larges size and have them all be that size"""

def padded_sequences(sequences):
  # So we extract the max length and use that one. Shorter sequences will just use 0's where they don't have words
    max_sequence_length = max([len(x) for x in sequences])
    # Now we have to reshape our sequences to fit to this new lentgh
    # Thankfully keras has the function pad_sequences that does this
    # We then make it an array by calling np.array(padded_sequences)
    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_length, padding='pre'))
    
    # Now we split our sequences into data and labels
    # for the phrase "hello new world"
    # we will have the seuqences and labels: 
    # hello -> new
    # hello new -> world
    # Where each label is the next word we are trying to predict based on the sequence
    data = sequences[:,:-1]
    # So our data will be all the words up to the last one
    label = sequences[:,-1]
    # Our label will be our last word
    # We don't want to assign greater importance to certain words just because they have a bigger number
    # So we make them all arrays of 0 and 1. 
    # Each one of our labels will have a specific value
    # i.e, hello can be [0, 0, 0, ..... ,  1] 
    # the length depends on the number of words we can have
    
    label = ku.to_categorical(label, num_classes=total_words)
    return data, label, max_sequence_length

"""Let's do it! But as we can see our token_conv variable is very very large, let's try to add only the first 30,000 sequences (So our program can run)"""

padded_conv, label_conv, max_sequence_length = padded_sequences(token_conv[:30000])

"""Let's see what our first sequence looks like. Toy around with it and you can see we added "padding" before our integers (letters) therefore making them all the same size"""

print(padded_conv[3])

"""Now it's time to build our model!"""

# Declare a sequential model
model = Sequential()
# Add a layer to the model (Embedding) that will allow us to take the inputs
model.add(Embedding(total_words, 10, input_length=max_sequence_length - 1)) # because its not 0-based
# Add an LSTM Layer with 100 units
model.add(LSTM(100))
model.add(Dropout(0.2))

# Add another layer (our output layer) with softmax actiavtion
model.add(Dense(total_words, activation='softmax'))
# Compile model with adam optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam')

"""See what our model looks like! with the summary option"""

model.summary()

"""That's a lot of parameters!

Now let's train our model! We add as parameters our padded_conv (Our padded sequences, our training data) and our label_conv (which are also padded and are our target data)
"""

model.fit(padded_conv, label_conv, epochs=100, verbose=2)

"""Now try our chatbot! It won't be perfect, we would need to add more data and more layers but it works very good nonetheless!"""

def generate_text(seed_text, model, max_sequence_len):
    for _ in range(50):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = model.predict_classes(token_list, verbose=0)
        
        output_word = ""
        for word,index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " "+output_word
    return seed_text.title()

print (generate_text("What is the capital of Peru?", model, max_sequence_length))

